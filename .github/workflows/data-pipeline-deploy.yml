name: Deploy Data Pipeline to GCP VM

on:
  push:
    paths:
      - 'data/data-pipeline/**'
  workflow_dispatch:  # Allow manual triggering

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      PROJECT_ID: data-pipeline-deployment-trial
      ZONE: us-east1-d
      VM_NAME: airflow-vm
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Authenticate to Google Cloud
        id: 'auth'
        uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: '${{ secrets.GCP_SA_KEY }}'
          
      - name: 'Set up Cloud SDK'
        uses: 'google-github-actions/setup-gcloud@v2'
      
      - name: Create SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.GCP_SSH_PRIVATE_KEY }}" > ~/.ssh/gcp_airflow_key
          chmod 600 ~/.ssh/gcp_airflow_key
          
      - name: Get VM IP address
        id: get-ip
        run: |
          IP=$(gcloud compute addresses describe airflow-access-ip --region=us-east1 --format='get(address)')
          echo "VM_IP=$IP" >> $GITHUB_ENV
          
      - name: Deploy to VM
        run: |
          # Create deployment script
          cd /home
          cd /rixirx
          cat > deploy.sh << 'EOL'
          #!/bin/bash
          set -euxo pipefail
          # Check if repository exists and update, fail if not present
          if [ -d "AI-GovBot" ]; then
            echo "Repository exists, updating..."
            cd AI-GovBot
            git fetch
            git pull
          else
            echo "ERROR: Repository doesn't exist on the VM. Please manually set up the VM first."
            echo "Use the setup_airflow_vm.sh script to initialize the environment."
            exit 1
          fi
          
          # Go to pipeline directory
          cd data/data-pipeline
          
          # Stop any running Docker containers
          echo "Stopping any running Docker containers..."
          docker compose down || true
          
          # Ensure directories exist
          mkdir -p ./logs ./plugins ./secrets
          
          # Check if environment file exists, fail if not
          if [ ! -f .env ]; then
            echo "ERROR: .env file not found. Required for pipeline operation."
            exit 1
          fi
          
          # Check if secrets file exists, fail if not
          if [ ! -f ./secrets/google_cloud_key.json ]; then
            echo "ERROR: google_cloud_key.json not found. Required for pipeline operation."
            exit 1
          fi
          
          # Build and start Docker containers
          echo "Building and starting Docker containers..."
          docker compose build
          docker compose up -d
          
          # Unpause and trigger the DAG
          echo "Unpausing and triggering DAG: Data_pipeline_HARVEY"
          docker compose exec -T airflow-webserver airflow dags unpause Data_pipeline_HARVEY
          docker compose exec -T airflow-webserver airflow dags trigger Data_pipeline_HARVEY
          
          echo "âœ… Deployment completed successfully!"
          EOL
          
          # Make script executable
          chmod +x deploy.sh
          
          # Copy script to VM
          gcloud compute scp deploy.sh $VM_NAME:~/deploy.sh --zone=$ZONE
          
          # Execute script on VM
          gcloud compute ssh $VM_NAME --zone=$ZONE -- "sudo bash ~/deploy.sh"
          
          echo "ðŸŽ‰ Data pipeline deployment complete!"
          echo "ðŸŒ Access Airflow UI at: http://$VM_IP:8080"